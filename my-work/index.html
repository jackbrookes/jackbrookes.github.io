<!doctype html>
<html>

<head>
    <link rel="stylesheet" href="/static/styles.css">
    <link rel="stylesheet" href="/static/pygments.css">
    <link rel="shortcut icon" type="image/png" href="/static/me-circ.png" />
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="Homepage of Jack Brookes" />
    <meta property="article:published_time" content="" />

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@jackbrookes" />
    <meta name="twitter:creator" content="@jackbrookes" />
    <meta name="twitter:title" content="My Work - Jack Brookes" />
    <meta name="twitter:description" content="Homepage of Jack Brookes" />
    <meta name="twitter:image" content="https://jbrookes.com/static/jbrookes-screenshot.png" />

    <title>My Work â€” Jack Brookes</title>

</head>

<body>

    <div id="container">
        <div id="sidebar">
            <nav>
                <a class="header" href="/">Jack Brookes</a>
                <ul id="links">
                    <li>
                        <a href="/">Index</a>
                    </li>
                    <li>
                        <a href="/my-work">My work</a>
                    </li>
                    <li>
                        <a href="/posts">Posts</a>
                    </li>
                    <li>
                        <a href="https://twitter.com/jackbrookes">Twitter</a>
                    </li>
                    <li>
                        <a href="https://github.com/jackbrookes">GitHub</a>
                    </li>
                    <li>
                        <a href="/contact">Contact</a>
                    </li>
                </ul>
            </nav>
            &nbsp;
        </div>

        <div id="content">
            <h1 id="my-work">My Work</h1>

<h2 id="software-portfolio">Software portfolio</h2>

<p>My open sourced work can be found at my <a href="http://github.com/jackbrookes">GitHub</a>.</p>

<h2 id="research-publications">Research publications</h2>

<p>Up-to-date list available on <a href="https://scholar.google.com/citations?user=0kwtpyoAAAAJ&amp;hl=en">Google Scholar</a>.</p>

<h2 id="projects">Projects</h2>

<p>The <a href="http://immersivecognition.github.io/unity-experiment-framework">Unity Experiment Framework (UXF)</a> is a software framework for developing virtual reality human behaviour experiments in Unity. It gives researchers easy-to-use tools to create applications to investigate how humans learn, move, and think. UXF includes complex systems for experiment flow and data collection in a set of simple Unity components. It supports virtual reality experiments, web browser based experiments, traditional screen-based experiments, and remote data collection by storing data in the cloud. I developed it during my PhD, made it open source, and have been continually updating it since then. </p>

<p><img src="/static/image/uxf.png"/></p>

<p>Core skills &amp; technologies:</p>

<ul>
<li>Unity C# API design</li>
<li>Custom Unity Editor tools</li>
<li>Multithreaded file I/O</li>
<li>Communication with cloud databases (AWS)</li>
<li>WebGL</li>
<li>Dynamic user interfaces</li>
<li>Unit testing</li>
<li>Extensive documentation</li>
</ul>

<p>During my PhD and beyond I have developed several virtual reality experiments to examine human behaviour, mostly in the area of human sensorimotor decision making. All were developed in Unity and with UXF.</p>

<h3 id="vr-threat-toolkit-ongoing">VR Threat Toolkit (Ongoing)</h3>

<p>I am currently working at UCL building a Unity platform for studying human movement within threatening VR scenarios. Alongside building statistical models of cognitive processes, this allows us to study how humans act so quickly in the face of fear, despite the complexity of the required movements. With this platform, we can easily create sequences of pre-planned events, including interactive elements, which allows each participant to experience a controlled encounter with a virtual threat, whilst we collect movement and physiological data.</p>

<p><img src="/static/image/vrthreat-panther.jpg" onclick="window.open(this.src, '_blank').focus()" style="cursor: pointer;"/></p>

<p>Core skills &amp; technologies:</p>

<ul>
<li>Virtual Reality physics &amp; interaction</li>
<li>Procedural animation (i.e. movement of animal 3D models)</li>
<li>Custom shaders</li>
<li>Performance optimisation (material combining, static batching, occlusion culling)</li>
<li>Complex coroutine systems for dynamic event sequencing </li>
<li>Custom Unity Editor tools</li>
<li>Spatialised virtual audio</li>
<li>Screen, camera, and microphone recording from Unity</li>
<li>Unity Post Processing Stack v2 </li>
</ul>

<h3 id="vr-ar-surgical-training">VR / AR Surgical training</h3>

<p>Part of my PhD involved research into how surgical simulator systems could be improved with haptic technology in order to enhance learning outcomes. I developed algorithms to apply assistive or disruptive forces to allow for practice matching an individual's skill level.</p>

<p>I also developed software for the MOOG Simodont Dental Trainer, which replicates the haptic sensations of many dental procedures. My software allows creation of "tasks" - blocks of virtual material with different levels of softness, which trainee dentists must learn to discriminate and drill out. My software is available <a href="https://github.com/jackbrookes/simodont-model-builder">open source</a>.</p>

<p><img src="/static/image/simodont.bmp" width="500" onclick="window.open(this.src, '_blank').focus()" style="cursor: pointer;"/> 
<img src="/static/image/smb.PNG" width="500" onclick="window.open(this.src, '_blank').focus()" style="cursor: pointer;"/> </p>

<h3 id="interceptive-timing">Interceptive timing</h3>

<p>This task examines the interceptive timing ability of children and adults. Crucially, VR allows the task to be made 3D - which means we can more reliably discern spatial and temporal errors separately. The gamification of the interceptive timing task means children enjoy taking part in the studies.</p>

<p><video controls width="500" muted>
  <source src="/static/image/interceptive-timing-vr.mp4" type="video/mp4">
  Your browser does not support HTML5 video.
</video></p>

<h3 id="golf-training">Golf training</h3>

<p>A great challenge in learning is understanding generalisation. Humans and other animals manage to quickly generalise abilities learned in one domain to another one. Virtual Reality poses a massive opportunity for training of skills ranging from surgery to customer service. For this to be useful, the skills learned in VR need to transfer to the real world. We are using this virtual reality golf task I developed to examine how we generalise skills learned in one medium to another.</p>

<p><img src="/static/image/golf.png" width="500" onclick="window.open(this.src, '_blank').focus()" style="cursor: pointer;"/> </p>

<h3 id="intrinsic-vs-extrinsic-costs-in-sensorimotor-decision-making">Intrinsic vs extrinsic costs in sensorimotor decision making</h3>

<p>Here we present participants with a series of choices between two objects. These objects differ in their <em>extrinsic</em> costs in terms of number of stars (i.e. points), and also their <em>intrinsic</em> costs because they have certain distance and timing constraints.</p>

<p><img src="/static/image/bubbles.jpg" width="500" onclick="window.open(this.src, '_blank').focus()" style="cursor: pointer;"/></p>

<h3 id="postural-sway-assessment-tool">Postural sway assessment tool</h3>

<p>This task measures the participant's head position across three conditions: vision, no vision, and the oscillating room intervention. VR makes these manipulations easy, and allows researchers to examine the participants ability to use vision to adjust balance. This project is available free &amp; <a href="https://github.com/immersivecognition/posture-assessment-vr">open source</a>. </p>

<p><video controls width="500" muted>
  <source src="/static/image/sway.mp4" type="video/mp4">
  Your browser does not support HTML5 video.
</video></p>

<h3 id="action-bandits">Action-bandits</h3>

<p>In my PhD I study how different types of errors (selection error, movement error) change the way we learn about our world. In this experiment, participants must physically swipe through one of two targets. Virtual Reality allows me to easily manipulate the rate of movement errors the participant believes they are making, by hiding the position of the hand when needed.  </p>

<p><video controls width="500" muted>
  <source src="/static/image/action-bandits.mp4" type="video/mp4">
  Your browser does not support HTML5 video.
</video></p>

<h3 id="prehension">Prehension</h3>

<p>Reach-to-grasp behaviours have been studied for decades, but only recently VR has allowed new experiments to be created. Here, we examine the effects of removing haptics or vision of the hand when reaching for an object using the VR experiment I developed.</p>

<p><video controls width="500" muted>
  <source src="/static/image/prehension-vr.mp4" type="video/mp4">
  Your browser does not support HTML5 video.
</video></p>

<h3 id="robotic-rehabilitation">Robotic rehabilitation</h3>

<p>During my Masters project I developed a snake clone which is played by a robotic arm that communicates with Unity using real-time multiplayer networking technologies (TCP/UDP).</p>

<p><video controls width="500" muted>
  <source src="/static/image/snake.mp4" type="video/mp4">
  Your browser does not support HTML5 video.
</video></p>

<h3 id="haptic-handwriting">Haptic Handwriting</h3>

<p>Handwriting is an important life skill, but many children underperform. Here I developed a simple handwriting task that allows children to virtually practice handwriting with haptic interventions that potentially improve learning rate. The Unity application communicates to a Phantom Omni haptic pen system. We tested this in a primary school, which was then featured on the BBC Inside Out program.</p>

<p><img src="/static/image/handwriting.png" width="500" onclick="window.open(this.src, '_blank').focus()" style="cursor: pointer;"/> 
<img src="/static/image/bbc-vr-children-handwriting.png" width="500" onclick="window.open(this.src, '_blank').focus()" style="cursor: pointer;"/> </p>

<h3 id="treasure-chest-bandits">Treasure chest bandits</h3>

<p>Continuing the work done in my PhD on learning in the context of movement errors, I developed a similar task that removed the motor aspect of the task, and transformed it into a 2-stage decision making task. This is done on a computer monitor rather than in VR.</p>

<p><video controls width="500" muted>
  <source src="/static/image/treasure.mp4" type="video/mp4">
  Your browser does not support HTML5 video.
</video></p>

<h3 id="visuomotor-adaptation">Visuomotor adaptation</h3>

<p>A common motor learning experiment paradigm involves applying a transformation to the environment, such that movements are mapped to a cursor position in a novel way. Here I developed a VR version of this task, which opens up the possibility of developing interesting new interventions, and running these types of experiments on a larger scale.</p>

<p><video controls width="500" muted>
  <source src="/static/image/vmr.mp4" type="video/mp4">
  Your browser does not support HTML5 video.
</video></p>

<h3 id="paired-associates-memorisation">Paired associates memorisation</h3>

<p>Memorising a pair of objects, and then recalling the second object after being prompted with the first, is a common way of measuring memory. I developed a VR version of this task, which uses realistic 3D objects rather than the classic image or word based tests in an attempt to be more ecologically valid. We use this task to examine the effects of sleep interventions on memory performance.</p>

<p><video controls width="500" muted>
  <source src="/static/image/paired-associates.mp4" type="video/mp4">
  Your browser does not support HTML5 video.
</video></p>

<h1 id="other-3dvr-software">Other 3D/VR software:</h1>

<ul>
<li><a href="https://github.com/immersivecognition/vr-demo-pack">VR Demo Pack</a> - Unity package that allows developers to add an avatar, viewable in 3rd person, to their task for presentation and video purposes.</li>
<li><a href="https://github.com/jackbrookes/bouncebeat">BounceBeat</a> - VR musical physics sandbox game made in Unity.</li>
<li><a href="https://github.com/jackbrookes/vr-graph-intersection">Unnamed concept VR game</a> - VR puzzle game where the goal is to create faces from nodes that do not create intersections in a 3D graph.</li>
<li><a href="https://github.com/jackbrookes/eyeballs-vr-demo">Eyeballs VR Demo</a> - VR demo made in Unity that allows you to detach your eyes from your head.</li>
<li><a href="https://jackbrookes.itch.io/isometricism">Isometricism</a> - Concept puzzle game where the visual perspective alters the paths the player can take.</li>
</ul>

<p><video controls width="500" muted>
  <source src="/static/image/isometricism.mp4" type="video/mp4">
  Your browser does not support HTML5 video.
</video></p>

<ul>
<li><a href="https://github.com/jackbrookes/halo-reach-custom-gametypes">Halo custom gametype scripts</a> - Collection of my custom scripted gametypes for Halo: Reach, written in a Lua-like language.</li>
</ul>
 
            <div id="footer">Jack Brookes &#169;2021</div>
        </div>
    </div>

    <!-- Default Statcounter code for jbrookes.com http://jbrookes.com -->
    <script type="text/javascript">
        var sc_project = 12132297;
        var sc_invisible = 1;
        var sc_security = "9e8bf11f";
        var sc_https = 1;
        var sc_remove_link = 1;
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter">
            <img class="statcounter" src="https://c.statcounter.com/12132297/0/9e8bf11f/1/" alt="Web Analytics">
        </div>
    </noscript>
    <!-- End of Statcounter Code -->

</body>

</html>